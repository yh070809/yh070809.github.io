---
layout:     post
title:      ML  Computer  Vision 02
subtitle:   ML 笔记总结 04
date:       2020-01-24
author:     Kaiyun
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Machine Learning
---
照片分为黑白和彩色 在图像里我们相应的有灰度图像和彩色图像对于灰度图像由于只有明暗的区别
只需要一个数字就可以表示不同的灰度

通常我们用0表示最暗的黑色，255 表示最亮的白色
一张彩色图可以由一个整数组成的立方体阵列来表示
我们称这样按立方体排列的数字阵列为三阶张量 tensor
这个三阶张量的长度与宽度即为图像的分辨率 高度为3 
对数字图像而言 三阶张量的高度也称为通道 （channel ）数 因此我们也说彩色图像有三个通道
矩阵可以看做是高度为1 的三阶张量因此灰度图像只有一个通道

张量是数学，物理及工程等学科中的一个基本概念，我们之前遇到的许多概念都是张量的特殊形式
例如标量 scalar 属于零阶张量 向量是一阶张量  而矩阵则是二阶张量

在深度学习之前，图像特征的设计 一直是计算机是觉得领域中一个重要的研究课题
在这个领域发展的初期， 人们手工设计了各种图像特征 
这些特征可以描述图像的颜色， 边缘 纹理 等基本性质
结合机器学习技术 能解决物体识别和物体检测等实际问题

既然图像在计算机中可以表示成三阶向量 那么从图像中提取特征便是对这个三阶张量进行运算的过程
其中非常重要的一种运算时卷积

卷积像加减乘除一样是一种数学运算
参与卷积运算的可以是向量矩阵或是三阶张量
我们先从向量的卷积入手
两个向量卷积的结果任然是一个向量 他的计算过程如图所示
我们先将两个向量的第一个元素对齐  并截去长向量中多余的元素
然后 我们计算这两个维数相同的向量的內积 并将算得的结果作为结果向量的第一个元素

接下来我们将短向量向下滑动一个元素  从原始的长向量中截去不能预知对应的元素

并计算內积  重复 “滑动 -> 截取 -> 计算內积” 这个过程
直到短向量的最后一个元素和长向量的最后一个元素对齐为止  最后就可以得到这两个向量卷积的结果
作为一种特殊情形， 当两个向量的长度相同时 不需要进行滑动操作 卷积结果是长度为1的向量 结果向量中这个元素就是两个向量的內积

从上面的定义可知  卷积结果的维数通常比长向量低 有时候我们为了使得卷积之后维数和长向量一致 会在长向量的两端补上一些0，
类似的  我们可以定义矩阵的卷积 在此之前 我们首先需要将内及运算拓展到矩阵上   对于两个形状相同的矩阵 他们的內积是每个对应位置的数字相乘之后的和

进行向量的卷积时， 我们只需要沿着一个方向进行滑动， 而进行矩阵的卷积时 我们需要沿着横向和纵向两个方向进行滑动

卷积运算在图像处理中应用十分广泛， 许多图像特征提取方法都会用到卷积， 以灰度图为例，我们知道在计算机中一副灰度图像被表示为一个整数的矩阵

如果我们用一个形状较小的矩阵和这个图像矩阵做卷积运算，就可以得到一个新的矩阵
这个新的矩阵就可以看成是一副新的图像
换句话说， 通过卷积运算   我们可以将原图像变换为一幅新的图像
这幅图像有时候比原图像更清楚的表示了某些性质， 我们可以把它当做原图像的一个特征
这里用的小矩阵就称为卷积核 通常图像矩阵中的元素都是介于 0- 255 之间的整数，但是卷积核中的元素可以是任意实数

通过卷积 我们可以从图像中提取边缘特征 
在没有边缘的比较平坦的区域， 图像像素值的变化比较小，而横向边缘上下两侧的像素差异明显， 竖向边缘左右两侧两侧的像素也会有较大差别

更进一步的， 研究者们设计了一些更加复杂而有效的特征 方向梯度直方图 （HOG） 是一种经典的图像特征 在物体识别和动物检测中有较好的应用  
方向梯度直方图使用边缘检测技术 和一些统计学的方法， 可以表示出图像中物体的轮廓， 由于不同的物体轮廓有所不同， 因此我们可以利用方向梯度直方图特征区分图像中的不同物体

方向梯度直方图的提取过程主要包括两个步骤 ， 首先我们利用卷积运算从图像中提取边缘特征， 接下来我们将图片划分成若干区域， 并对边缘特诊按照方向和幅度进行统计， 并形成直方图， 最后我们将所有区域内的直方图拼接起来，就形成了特征向量



深度神经网络通常由多个顺序链接的层组成， 第一层一般以图像为输入， 通过特定的运算从图像中提取特征， 接下来每一层以前一层提取出来的特征输入
对其进行特定形式的变化， 变可以得到一些更加复杂的特征
这种层次化的特征提取过程可以累加，赋予神经网络枪法的特征提取能力
经过很多层的变换之后， 神经网络就可以将原始图像变换为高层次的抽象特征

这种由简单到复杂，由低级到高级的抽象过程可以通过生活中的例子来体会，例如 在英语的学习过程中 通过字母的组合 可以得到单词
通过单词的组合 可以得到句子  通过句子的分析 可以了解语义  通过语义的分析 可以获得表达的思想或者目的 
这种语义 思想等就是更高级别的抽象


接下来  我们看一个具体的神经网络的例子
对深度神经网络有一个直观的感受  这个网络中出现了卷积层 ， ReLU非线性活跃层 池化层 全连接层
softmax 归一化指数层等概念


卷积层 
卷积层是深度神经网络在处理图像是十分常用的一种层
当一个深度神经网络以卷积层为主体的时候 我们也称之为卷积神经网络

神经网络中的卷积层就是用卷积运算对原始图像或者上一层的特征进行变换的层 在上一节中我们学习了边缘特征的提取 知道一种特定的卷积核可以对图像进行
一种特殊的变换， 从而提取出某种特定的特征， 如横向边缘或者纵向边缘 在一个卷积层中 为了从图像中提取多种形式的特征 ，我们通常 使用多个卷积核对输入的图像进行不同的卷积操作 一个卷积核可以得到一个通道为1 的三阶张量， 多个卷积核就可以得到多个通道为1 的三阶张量， 我们把这些结果作为不同的通道组合起来 就可以得到一个新的三阶张量 这个三阶张量的通道数就等于我们使用的卷积核的个数  由于每一个通道都是从原图像中提取的一种特征 我们也将这个三阶张量称为特征图  这个特征图就是卷积层的最终输出



特征图与彩色图都是三阶张量 也有若干个通道 ，因此卷积层不仅可以作用于图像 也可以作用于其他输出层的特征图 
通常，一个深度神经网络的第一个卷积层会以图像作为输入，而之后的卷积层会以前面的层输出的特征图为输入


全连接层 
在图片分类问题中 输入图片在经过若干卷积层之后， 会将得到的特征图转换为特征向量 如果需要对这个特征向量进行变换 经常用到的便是全连接层
在全连接层中， 我们会使用若干维数相同的向量与输入向量做內积操作， 并将所有结果拼接成一个向量作为输出， 具体来说， 如果一个全连接层以向量X作为输入，
我们会用总共K 个维数想用的参数向量Wk 与 X 做內积操作， 再在每个结果上加一个标量bx ，即完成yk = X * Wk + bk 的运算， 最后 我们将K个标量结果Yk组成向量Y 作为这一层的输出



归一化指数 
归一化指数的作用就是完成多类线性分类器中的归一化指数函数的计算， 具体来说， 对于输入向量 X = （x1,x2,x3,.....xn）, 计算n个标量值 Yk = （指数函数） 并将它们拼接成向量Y = （y1,y2,y3, ...., yn）作为输出， 归一化指数层一般是分类网络的最后一层 ，它以一个长度和类别个数相等的特征向量作为输入 （这个特征向量通常来自一个全连接层的输出） 然后 输出图像属于各个类别的概率



非线性池化层

通常 我们需要再每个卷积层和全连接层后面连接一个非线性激活层（non-activation layer） 为什么呢？ 其实不管是卷积运算还是全连接层中的运算
它们都是关于自变量的函数 即所谓的线性函数 （linear function） 线性函数有一个性质： 若干线性计算的复合仍然是线性的 换句话说 如果我们只是将卷积层和全连接层直接堆叠起来 那么它们对输入图片产生的效果就可以被一个全连接层替代 
这样一来 我们就堆叠了很多层 但是每一层的变换效果实际上被合并到了一起  而如果我们在每次线性运算后， 再进行一次非线性运算 那么每次变换的效果就可以得到保留  非线性激活层的形式有许多种 他们的基本形式是先选定某种非线性函数 然后在对输入特征图或者特征向量的每一个元素应用这种非线性函数，得到输出

以线性整流函数构成的非线性激活层（简称ReLU 层）为例 对于输入的特征向量或者特征图 
它会将其小于零的元素变成零 而保持其余元素的值不变，就得到了输入， 因为ReLU的计算非常简单  所以它的计算速度往往比其他非线性激活层快很多 
加之其在实际应用中的效果也很好 因此在深度神经网络中被广泛的使用


池化层
在计算卷积的时候， 我们会用卷积核划过图像或特征图的每一个像素 如果图像或特征图的分辨率很大 那么卷积层的计算量就会很大 
为了解决这个问题我们通常在几个卷积层之后插入池化层（pooling layer） 以降低特征图的分辨率

池化层的赤化操作步骤如下
首先 我们将特征图按照通道分开， 得到若干个矩阵
对于每个矩阵我们将其切割成若干个大小相等的正方形小块 例 我们将一个4 X 4 的矩阵分割成4个正方形区块， 每个区块的大小为 2 X 2  接下来
我们对每一个区块取最大值或者平均值，并组成新的矩阵，最后 我们将所有通道的结果矩阵按原则顺序堆叠起来形成一个三阶张量 这个三阶张量就是池化层的输出

对每一个区块取最大值的池化层， 我们称之为最大池化层 而取平均值得池化层称之为平均池化层
经过池化后 特征图的长和宽都会减小到原来的1/2 特征图中的元素数目减小到原来的1/4  通常我们会在卷积层之后增加池化层 这样， 在经过若干卷积， 池化层的组合之后 在不考虑通道数的情况下， 特征图的分辨率就会远小于输入图像的分辨率，大大减小了对计算量和参数数量的需求


反向传播算法

是训练神经网络最有效的手段之一， 每次 我们将一副训练图像输入网络中， 经过逐层的计算， 最终得到的预测属于每一类的概率 
我们将预测结果与正确答案进行对比， 如果发现预测结果不够友好，那么会从最后一层开始， 逐层调整神经网络的参数， 使得网络对这个训练样本能够做出更好的预测
我们将这种从后往前调整参数的方法称为 反向传播算法具体的调整算法设计梯度计算的链式法则 和随机梯度下降等复杂的知识





 









